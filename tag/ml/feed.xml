<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://whitenoize-snu.github.io/tag/ml/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://whitenoize-snu.github.io/" rel="alternate" type="text/html" />
  <updated>2021-11-04T16:40:44+09:00</updated>
  <id>https://whitenoize-snu.github.io/tag/ml/feed.xml</id>

  
  
  

  
    <title type="html">IT ê³µë¶€ë°© | </title>
  

  
    <subtitle>ê°œë°œ ì§€ì‹ ë°°ìš´ ê²ƒë“¤ ë„ì ë„ì </subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Machine Learning - Regression</title>
      <link href="https://whitenoize-snu.github.io/ml-regression" rel="alternate" type="text/html" title="Machine Learning - Regression" />
      <published>2021-11-03T10:40:00+09:00</published>
      <updated>2021-11-03T10:40:00+09:00</updated>
      <id>https://whitenoize-snu.github.io/ml-regression</id>
      <content type="html" xml:base="https://whitenoize-snu.github.io/ml-regression">&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;Machine Learning ê¸°ì´ˆ &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./ml-regression&quot;&gt;Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;regression&quot;&gt;Regression&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;linear-regression&quot;&gt;Linear Regression&lt;/h2&gt;
&lt;p&gt;ê°€ì¥ ê°„ë‹¨í•œ Linear Regression ëª¨ë¸ì€ ë‹¤ìŒ ì‹ìœ¼ë¡œ í‘œí˜„ëœë‹¤.&lt;/p&gt;

\[\begin{equation}
\hat{y} = \theta_0+\theta_1x_1+\cdots+\theta_px_p
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;\(\hat{y}\) : prediction&lt;/li&gt;
  &lt;li&gt;\(p\) : # of features&lt;/li&gt;
  &lt;li&gt;\(x_i\) : \(i_{th}\) feature&lt;/li&gt;
  &lt;li&gt;\(\theta_j\) : \(j_{th}\) model parameter (\(\theta_0\) for bias, \(\theta_1, \theta_2, \cdots \theta_p\) for weights)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ë²¡í„° í‘œê¸°ë¥¼ ì´ìš©í•˜ë©´ ë”ìš± ê°„ë‹¨í•˜ê²Œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.&lt;/p&gt;

\[\begin{equation}
\hat{y} = h_{\boldsymbol{\theta}}(\mathbf{x}) = \boldsymbol{\theta}\cdot\mathbf{x}
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;\(\boldsymbol{\theta}=[\theta_0, \theta_1, \cdots, \theta_p]\) : Model parameter vector including bias &amp;amp; weights&lt;/li&gt;
  &lt;li&gt;\(\mathbf{x}=[x_0, x_1, \cdots, x_p]\) : Feature vector, \(x_0=1\)&lt;/li&gt;
  &lt;li&gt;\(h_{\boldsymbol{\theta}}\) : Hypothesis function with parameter \(\boldsymbol{\theta}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì´ì œ \(n\)ê°œì˜ í›ˆë ¨ ì„¸íŠ¸ê°€ ìˆë‹¤ê³  í•´ë³´ì. ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„° ì„¸íŠ¸ëŠ” í–‰ì´ ê° ë°ì´í„° ë ˆì´ë¸”ì„ ì˜ë¯¸í•˜ê³  ì—´ì€ íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ë¯€ë¡œ í›ˆë ¨ ì„¸íŠ¸ \(\mathbf{X}\)ëŠ” \(\mathbf{X}=[\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \cdots, \mathbf{x}^{(n)}]^T\)ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ìš°ë¦¬ì˜ ëª©í‘œëŠ” í•´ë‹¹ ë°ì´í„° ì„¸íŠ¸ê°€ ìš°ë¦¬ì˜ ëª¨ë¸ì— ì–¼ë§ˆë‚˜ ì˜ ë“¤ì–´ë§ëŠ”ì§€ ì•Œì•„ë‚´ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ ì§€í‘œë¡œ ìì£¼ ì‚¬ìš© ë˜ëŠ” ê²ƒìœ¼ë¡œ &lt;strong&gt;í‰ê·  ì œê³± ì˜¤ì°¨ (MSE)&lt;/strong&gt;ê°€ ìˆë‹¤.&lt;/p&gt;

\[\begin{equation}
MSE\left (\mathbf{X}, h_{\boldsymbol{\theta}} \right)=\frac{1}{n}\sum_{i=1}^{n}\left(\boldsymbol{\theta}^T\mathbf{x}^{(i)}-y^{(i)} \right) ^ 2
\end{equation}\]

&lt;h3 id=&quot;normal-equation&quot;&gt;Normal Equation&lt;/h3&gt;
&lt;p&gt;ë¹„ìš© í•¨ìˆ˜ë¥¼ ìµœì í™”í•˜ëŠ” \(\boldsymbol{\theta}=\boldsymbol{\hat{\theta}}\)ë¥¼ í•´ì„ì ìœ¼ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì„ &lt;strong&gt;ì •ê·œë°©ì •ì‹ (Normal Equation)&lt;/strong&gt;ì´ë¼ ë¶€ë¥¸ë‹¤.&lt;/p&gt;

\[\begin{equation}
\boldsymbol{\hat{\theta}}=\left(\mathbf{X}^T\mathbf{X} \right)^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;\(\boldsymbol{\hat{\theta}}\) : \(\boldsymbol{\theta}\) which minimizes the cost function&lt;/li&gt;
  &lt;li&gt;\(\mathbf{y}=[\mathbf{y}^{(1)}, \mathbf{y}^{(2)}, \cdots, \mathbf{y}^{(n)}]\) : target vector&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proof&quot;&gt;Proof&lt;/h3&gt;
&lt;p&gt;ì‹¤ì œ ì–»ëŠ” ë°ì´í„°ëŠ” ì•½ê°„ì˜ ì¡ìŒì´ ì„ì—¬ ìˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ ì‹ì— error termì„ í¬í•¨í•˜ë©´ ìš°ë¦¬ê°€ ì–»ì€ ë°ì´í„°ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

\[\begin{equation}
y_i = \theta_0 + \theta_1x_{i1} + \cdots + \theta_px_{ip} + \epsilon_i
\end{equation}\]

&lt;p&gt;í˜¹ì€&lt;/p&gt;

\[\begin{equation}
\mathbf{y} = \mathbf{X}\boldsymbol{\theta} + \boldsymbol{\epsilon}
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;\(\boldsymbol{\theta}=[\theta_0, \theta_1, \cdots, \theta_p]^T\) : Feature vector&lt;/li&gt;
  &lt;li&gt;\(\mathbf{y}=[y_1, y_2, \cdots, y_n]^T\)Â &lt;/li&gt;
  &lt;li&gt;\(\mathbf{X}=\)
\(\begin{bmatrix}
1 &amp;amp; x_{11} &amp;amp; \cdots&amp;amp; x_{ip}\\ 
1 &amp;amp; x_{21} &amp;amp; \cdots &amp;amp; x_{2p}\\ 
\vdots  &amp;amp; \vdots &amp;amp; \ddots  &amp;amp; \vdots\\ 
1 &amp;amp; x_{n1} &amp;amp; \cdots &amp;amp; x_{np}
\end{bmatrix}\)Â &lt;/li&gt;
&lt;/ul&gt;

\[\begin{equation}
\boldsymbol{\theta^*}=\underset{\boldsymbol{\theta}}{\operatorname{argmin}} \left \|\boldsymbol{\epsilon} \right \|^2 = \left \| \mathbf{y} - \mathbf{X} \boldsymbol{\theta} \right \| ^2
\end{equation}\]

&lt;p&gt;Errorë¥¼ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

\[\begin{align}
\left \| \mathbf{y} - \mathbf{X} \boldsymbol{\theta} \right \| ^2 &amp;amp;= \left ( \mathbf{y} - \mathbf{X} \boldsymbol{\theta} \right ) ^T \left ( \mathbf{y}-\mathbf{X}\boldsymbol{\theta} \right ) \\
&amp;amp;=  \left ( \mathbf{y}^T - \boldsymbol{\theta}^T \mathbf{X}^T \right ) \left ( \mathbf{y}-\mathbf{X}\boldsymbol{\theta} \right )\\
&amp;amp;= \mathbf{y}^T \mathbf{y} - \mathbf{y}^T \mathbf{X} \boldsymbol{\theta} - \boldsymbol{\theta}^T \mathbf{X}^T \mathbf{y} + \boldsymbol{\theta}^T \mathbf{X}^T \mathbf{X} \boldsymbol{\theta} \\
\end{align}\]

&lt;p&gt;Errorì˜ ìµœì†Œê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ ë¯¸ë¶„ë°©ì •ì‹ \(\nabla_{\boldsymbol{\theta}} \left \| \boldsymbol{\epsilon} \right \|^2 = 0\)ì„ í‘¼ë‹¤.&lt;/p&gt;

\[\begin{align}
\nabla_{\boldsymbol{\theta}} \left \| \mathbf{y} - \mathbf{X} \boldsymbol{\theta} \right \| &amp;amp;= -2\mathbf{y}^T \mathbf{X} + 2\boldsymbol{\theta}^T \mathbf{X}^T \mathbf{X} = 0 \\
\boldsymbol{\theta^*} &amp;amp;= \left(\mathbf{X}^T\mathbf{X} \right)^{-1}\mathbf{X}^T\mathbf{y}

\end{align}\]

&lt;p&gt;ì´í•´ë¥¼ ìœ„í•´ ê°€ì¥ ê°„ë‹¨í•œ ê²½ìš°ì¸ \(p=1\), ì¦‰ ì¢…ì†ë³€ìˆ˜ê°€ í•œê°œì¸ ê²½ìš°ì— ëŒ€í•´ ì‚´í´ë³´ì. ê·¸ë ‡ë‹¤ë©´ ëª¨ë¸ ì‹ ë° MSEë¥¼ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. í¸ì˜ë¥¼ ìœ„í•´ MSEë¥¼ ì†ì‹¤ í•¨ìˆ˜ Lossì˜ ì•ê¸€ìë¥¼ ë”°ì„œ Lì´ë¼ê³  í•˜ì˜€ë‹¤.&lt;/p&gt;

\[\begin{align}
y_i &amp;amp;= \theta_0 + \theta_1 x_i + \epsilon_i = \hat{y}_i + \epsilon_i \\
L   &amp;amp;= MSE(\mathbf{x}, \hat{y}_i) = \frac{1}{n}\sum_{i=1}^{n} \left ( y_i - \theta_0 - \theta_1x_i \right )^2
\end{align}\]

&lt;p&gt;Lossë¥¼ íŒŒë¼ë¯¸í„° \(\theta_0, \theta_1\)ì— ëŒ€í•´ ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

\[\begin{align}
\frac{\partial L}{\partial \theta_0} &amp;amp;= -\frac{2}{n} \sum \left ( y_i - \theta_0 - \theta_1 x_i \right ) = 0 \rightarrow n\theta_0 + \left (\sum x_i \right )\theta_1 = \sum y_i \\
\frac{\partial L}{\partial \theta_1} &amp;amp;= -\frac{2}{n} \sum x_i \left ( y_i - \theta_0 - \theta_1 x_i \right ) = 0 \rightarrow \left (\sum x_i \right )\theta_0 + \left (\sum x_i^2 \right )\theta_1 = \sum x_iy_i
\end{align}\]

&lt;p&gt;í–‰ë ¬ì„ ì´ìš©í•´ í‘œí˜„í•˜ë©´&lt;/p&gt;

\[\begin{equation}
\begin{bmatrix}
n &amp;amp; \sum x_i\\ 
\sum x_i &amp;amp; \sum x_i^2
\end{bmatrix}
\begin{bmatrix}
\theta_0\\ 
\theta_1
\end{bmatrix}
=
\begin{bmatrix}
\sum y_i\\ 
\sum x_iy_i
\end{bmatrix}
\end{equation}\]

\[\begin{equation}
\therefore
\begin{bmatrix}
\theta_0\\ 
\theta_1
\end{bmatrix}
=
\begin{bmatrix}
n &amp;amp; \sum x_i\\ 
\sum x_i &amp;amp; \sum x_i^2
\end{bmatrix} ^ {-1}
\begin{bmatrix}
\sum y_i\\ 
\sum x_iy_i
\end{bmatrix}

\end{equation}\]

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://atmos.washington.edu/~dennis/MatrixCalculus.pdf&quot;&gt;Matrix Calculus ì°¸ê³ ìë£Œ â†&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;
&lt;p&gt;ì •ê·œë°©ì •ì‹ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ \(y=4+3x\)ì— error term(Gaussian Noise)ì„ ë„£ì–´ ë¶„í¬ë¥¼ ë§Œë“¤ì—ˆë‹¤.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Data Generation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;\assets\ml-regression-randomlinear.png&quot; alt=&quot;Randomly generated linear dataset&quot; /&gt;
ì´ì œ ì •ê·œë°©ì •ì‹ì„ ì´ìš©í•´ \(\boldsymbol{\hat{\theta}}\)ì„ êµ¬í•´ë³¸ë‹¤.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Padding 1 to X
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;normalEquation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta_best&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_best&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;theta_best&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalEquation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_best&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.93250303&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.02749724&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ìš°ë¦¬ê°€ ì›í–ˆë˜ ê°’ì€ \(\theta_0=4, \theta_1=3\)ì´ì—ˆìœ¼ë‚˜ ì‹¤ì œë¡œ ì–»ì€ ê°’ì€ \(\theta_0=3.9325, \theta_1=3.0275\)ì´ë‹¤. ì¡ìŒì˜ ì˜í–¥ìœ¼ë¡œ ì¸í•´ ì•½ê°„ì˜ ì˜¤ì°¨ê°€ ë°œìƒí•œ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ì´ ê°’ì„ í† ëŒ€ë¡œ ìš°ë¦¬ì˜ ëª¨ë¸ì„ ê·¸ë˜í”„í•´ ë„ì‹œí•´ë³¸ë‹¤.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_new_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_new_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta_best&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r-'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'bo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$X_{1}$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Prediction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;\assets\ml-regression-randomlinear-2.png&quot; alt=&quot;Prediction of Linear Regression&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pythonì˜ Scikit-learn íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ê°„ë‹¨í•˜ê²Œ ì„ í˜• íšŒê·€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# ì¶”ê°€í•  ë¶€ë¶„
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;coefficient-of-determination&quot;&gt;Coefficient of Determination&lt;/h2&gt;
&lt;p&gt;Featureì˜ ê°œìˆ˜ \(m=1\)ì¼ ë•Œ ëª¨ë¸ì˜ ì í•©ì„±ì„ í‰ê°€í•˜ëŠ” ì§€í‘œë¡œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ ê²°ì •ê³„ìˆ˜ (Coefficient of Determination)ì´ë‹¤. ì§„í–‰í•˜ê¸°ì— ì•ì„œ ë°‘ì˜ ê·¸ë¦¼ê³¼ ì‹ì„ ì‚´í´ë³´ì.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\ml-regression-randomlinear-3.png&quot; alt=&quot;Coefficient of Determination&quot; /&gt;&lt;/p&gt;

\[\begin{align}
y_i - \bar{y} &amp;amp;= y_i - \hat{y}_i + \hat{y}_i - \bar{y} \\
              &amp;amp;= (y_i - \hat{y}_i) + (\hat{y}_i - \bar{y})
\end{align}\]

&lt;ul&gt;
  &lt;li&gt;\(y_i - \hat{y}_i\) : Difference &lt;strong&gt;not&lt;/strong&gt; explained by model&lt;/li&gt;
  &lt;li&gt;\(\hat{y}_i - \bar{y}\) : Difference explained by model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì¼ë°˜ì ìœ¼ë¡œ error termì€ í‰ê· ì´ 0ì¸ Gaussian Distributionì„ ë”°ë¥´ë¯€ë¡œ ë°ì´í„°ë¥¼ í‰ê· ë‚¼ ê²½ìš° error termì€ ì‚¬ë¼ì§„ë‹¤. ë”°ë¼ì„œ ì•Œê³  ìˆëŠ” ë°ì´í„°ì™€ í‰ê· ì˜ ì°¨ì´ë¥¼ ìœ„ì˜ ë‘ í•­ìœ¼ë¡œ ë¶„ë¦¬í•  ìˆ˜ ìˆë‹¤. ì´ë¡œë¶€í„° ê²°ì •ê³„ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ë©´ ë°˜ì‘ ë³€ìˆ˜ \(y_i\)ì˜ ë³€ë™ëŸ‰ ì¤‘ ëª¨í˜•ì„ í†µí•´ ì„¤ëª…ê°€ëŠ¥í•œ ë¶€ë¶„ì˜ ë¹„ìœ¨ì´ë¼ ë³¼ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

\[\begin{equation}
r^2 = \frac{SSM}{SST} = 1 - \frac{SSM}{SSE}
\end{equation}\]

&lt;p&gt;ì—¬ê¸°ì„œ&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(SST = \sum_{i=1}^{n}(y_i-\bar{y})^2\)Â &lt;/li&gt;
  &lt;li&gt;\(SSE = \sum_{i=1}^{n}(y_i-\hat{y}_i)^2\)Â &lt;/li&gt;
  &lt;li&gt;\(SSM = \sum_{i=1}^{n}(\hat{y}_i-\bar{y})^2\)Â &lt;/li&gt;
  &lt;li&gt;\(SST = SSE + SSM\) (ğŸ˜ Prove!)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prove&quot;&gt;Prove&lt;/h3&gt;
&lt;p&gt;\(\begin{align}
SST &amp;amp;= \sum \left (y_i - \bar{y} \right )^2 = \sum \left (y_i - \hat{y}_i + \hat{y}_i - \bar{y} \right )^2 = \sum \left ((y_i - \hat{y}_i) + (\hat{y}_i - \bar{y}) \right )^2 \\
    &amp;amp;= \sum \left (y_i - \hat{y}_i \right )^2 + \sum \left (\hat{y}_i - \bar{y} \right )^2 + 2\sum(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) \\
    &amp;amp;= SSE + SSM + 2 \times K
\end{align}\)&lt;/p&gt;

&lt;p&gt;\(K=0\)ì„ì„ ë³´ì´ì. ì•ì„œ ì‚¬ìš©í•œ ë°©ì‹ëŒ€ë¡œ error termì¸ SSEë¥¼ ìµœì†Œí™”í•˜ëŠ” \(\theta_0, \theta_1\)ì„ êµ¬í•˜ë©´ ëœë‹¤.&lt;/p&gt;

\[\begin{align}
\frac{\partial SSE}{\partial \theta_0} &amp;amp;= \frac{\partial (\sum \epsilon_i^2)}{\partial \theta_0}=2\sum\epsilon_i\frac{\partial \epsilon_i}{\partial \theta_0}=2\sum\epsilon_i(-1)=0 \rightarrow \sum\epsilon_i=0 \\
\sum \epsilon_i &amp;amp;= \sum y_i - n\theta_0 - \theta_1\sum x_i = 0 \rightarrow \theta_0 = \frac{1}{n}\sum y_i - \frac{\theta_1}{n}\sum x_i \\
\frac{\partial SSE}{\partial \theta_1} &amp;amp;= 2\sum\epsilon\frac{\partial \epsilon_i}{\partial \theta_1} = 2\sum \epsilon_i (-x_i) = 0 \rightarrow \sum x_i (y_i - \hat{y}_i) = 0 \\
x_i &amp;amp;= \frac{\hat{y}_i}{\theta_1} - \frac{\theta_0}{\theta_1} \rightarrow \sum \left ( \frac{\hat{y}_i}{\theta_1} - \frac{\theta_0}{\theta_1} \right ) \left ( y_i - \hat{y}_i \right ) = 0 \\
\end{align}\]

\[\begin{equation}
\frac{1}{\theta_1}\sum \hat{y}_i(y_i - \hat{y}_i) - \frac{\theta_0}{\theta_1}\sum(y_i - \hat{y}_i) = \frac{1}{\theta_1}\sum \hat{y}_i(y_i - \hat{y}_i) - \frac{\theta_0}{\theta_1}\sum \epsilon_i = 0 \\
\therefore \sum \hat{y}_i(y_i - \hat{y}_i) = 0
\end{equation}\]

&lt;p&gt;\(K\)ë¥¼ ë¶„ë°°í•˜ë©´ \(\sum \hat{y}_i(y_i - \hat{y}_i) - \bar{y}\sum(y_i - \hat{y}_i)\)ì´ë¯€ë¡œ ì•ì„  ê²°ê³¼ë¡œë¶€í„° \(K=0\)ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>WhiteNoize</name>
        
        
      </author>

      

      
        <category term="ml" />
      

      
        <summary type="html">Machine Learning ê¸°ì´ˆ Regression Regression Linear Regression ê°€ì¥ ê°„ë‹¨í•œ Linear Regression ëª¨ë¸ì€ ë‹¤ìŒ ì‹ìœ¼ë¡œ í‘œí˜„ëœë‹¤.</summary>
      

      
      
    </entry>
  
</feed>
